{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8400172,"sourceType":"datasetVersion","datasetId":4997913},{"sourceId":177388250,"sourceType":"kernelVersion"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"# Workflow essentials\nimport gc\nimport json\nimport torch\nimport random\nimport argparse\nfrom pathlib import Path\nfrom itertools import chain\nfrom functools import partial\nfrom datasets import Dataset, features\n\n# Data preprocessing and visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Model development\nfrom sklearn.metrics import f1_score\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import DataCollatorForTokenClassification","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:23:25.070318Z","iopub.execute_input":"2024-05-13T11:23:25.071017Z","iopub.status.idle":"2024-05-13T11:23:47.514878Z","shell.execute_reply.started":"2024-05-13T11:23:25.070977Z","shell.execute_reply":"2024-05-13T11:23:47.513450Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-13 11:23:37.518581: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-13 11:23:37.518716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-13 11:23:37.676030: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    # Path to test dataset\n    test_data = '/kaggle/input/pii-detection-removal-from-educational-data/test.json'\n    \n    # Path to saved model checkpoint\n    checkpoint = '/kaggle/input/pii-detection-deberta/deberta3base'\n    \n    # Data preprocessing\n    max_len = 5000\n    workers = 8\n    \n    # Model inference\n    threshold = 0.84\n    batch_size = 4\n    stride = 256\n    seed = 457","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:32:45.712105Z","iopub.execute_input":"2024-05-13T11:32:45.712521Z","iopub.status.idle":"2024-05-13T11:32:45.719212Z","shell.execute_reply.started":"2024-05-13T11:32:45.712492Z","shell.execute_reply":"2024-05-13T11:32:45.717877Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def global_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:32:46.723835Z","iopub.execute_input":"2024-05-13T11:32:46.724446Z","iopub.status.idle":"2024-05-13T11:32:46.730321Z","shell.execute_reply.started":"2024-05-13T11:32:46.724416Z","shell.execute_reply":"2024-05-13T11:32:46.729171Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Set seed for reproducibility across multiple libraries\nglobal_seed(CFG.seed)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:32:47.357801Z","iopub.execute_input":"2024-05-13T11:32:47.358928Z","iopub.status.idle":"2024-05-13T11:32:47.736810Z","shell.execute_reply.started":"2024-05-13T11:32:47.358877Z","shell.execute_reply":"2024-05-13T11:32:47.735596Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1750"},"metadata":{}}]},{"cell_type":"code","source":"class DataPreprocessor:\n    def tokenize(self, example, tokenizer):\n        # Initialize lists to store tokenized text and token map\n        text, token_map = [], []\n        \n        # Initialize index for token mapping\n        idx = 0\n\n        # Iterate over tokens and trailing whitespace in the example\n        for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n            # Append token to the text list\n            text.append(t)\n            \n            # Extend token map with index for the current token repeated for its length\n            token_map.extend([idx] * len(t))\n            \n            # If there is trailing whitespace after the token, append a space to the text and mark its position as -1 in the token map\n            if ws:\n                text.append(\" \")\n                token_map.append(-1)\n\n            # Increment the index for token mapping\n            idx += 1\n\n        # Tokenize the text using the provided tokenizer\n        tokenized = tokenizer(\"\".join(text), \n                              return_offsets_mapping=True,\n                              truncation=True,  \n                              max_length=CFG.max_len,  \n                              stride=CFG.stride,  \n                              return_overflowing_tokens=True)  \n\n        # Return the tokenized text along with the token map\n        return {\n            **tokenized, \n            \"token_map\": token_map, \n        }\n    \n# Initialize DataPreprocessor class\ndp = DataPreprocessor()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:32:47.853285Z","iopub.execute_input":"2024-05-13T11:32:47.853674Z","iopub.status.idle":"2024-05-13T11:32:47.861642Z","shell.execute_reply.started":"2024-05-13T11:32:47.853645Z","shell.execute_reply":"2024-05-13T11:32:47.860782Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Load test data\ndata = json.load(open(CFG.test_data))\n\n# Create the dataset\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],  \n    \"document\": [x[\"document\"] for x in data],  \n    \"tokens\": [x[\"tokens\"] for x in data],  \n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],  \n})\n\n# Initialize the tokenizer for the pre-trained model checkpoint\ntokenizer = AutoTokenizer.from_pretrained(CFG.checkpoint)\n\n# Tokenize the test essays\nds = ds.map(dp.tokenize, \n            fn_kwargs={\"tokenizer\": tokenizer},  \n            num_proc=CFG.workers) ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:32:48.305299Z","iopub.execute_input":"2024-05-13T11:32:48.305964Z","iopub.status.idle":"2024-05-13T11:32:50.979263Z","shell.execute_reply.started":"2024-05-13T11:32:48.305929Z","shell.execute_reply":"2024-05-13T11:32:50.977965Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=8):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5c8359e82bc4d9381e85433e35ba4b4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]},{"cell_type":"code","source":"class ModelInference:\n    def __init__(self):\n        pass\n\n    def backwards_map_preds(self, sub_predictions, max_len, CFG):\n        # nothing to map backwards if sequence is too short to be split in the first place\n        if max_len != 1:\n            for i in range(max_len):\n                if i == 0:\n                    # First sequence needs no SEP token (used to end a sequence)\n                    sub_predictions = sub_predictions[:,:-1,:]\n                elif i == max_len-1:\n                    # End sequence needs to CLS token + Stride tokens \n                    sub_predictions = sub_predictions[:,1+CFG.stride:,:] # CLS tokens + Stride tokens\n                else:\n                    # Middle sequence needs to CLS token + Stride tokens + SEP token\n                    sub_predictions = sub_predictions[:,1+CFG.stride:-1,:]\n                \n        return sub_predictions\n\n    def backwards_map_(self, row_attribute, max_len, CFG):\n        # Same logics as for backwards_map_preds - except lists instead of 3darray\n        if max_len != 1:\n            for i in range(max_len):\n                if i == 0:\n                    row_attribute = row_attribute[:-1]\n                elif i == max_len-1:\n                    row_attribute = row_attribute[1+CFG.stride:]\n                else:\n                    row_attribute = row_attribute[1+CFG.stride:-1]\n                \n        return row_attribute\n\n    def process_dataset(self, ds, tokenizer, CFG):\n        # Initialize model and collator\n        model = AutoModelForTokenClassification.from_pretrained(CFG.checkpoint)\n        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n        # Define training arguments\n        args = TrainingArguments(\n            \".\", \n            per_device_eval_batch_size=CFG.batch_size, \n            report_to=\"none\",\n        )\n\n        # Initialize Trainer\n        trainer = Trainer(\n            model=model, \n            args=args, \n            data_collator=collator, \n            tokenizer=tokenizer,\n        )\n\n        # Initialize empty lists and dictionary for storing predictions\n        preds = []\n        ds_dict = {\n            \"document\":[],\n            \"token_map\":[],\n            \"offset_mapping\":[],\n            \"tokens\":[]\n        }\n\n        # Iterate over each row in the dataset\n        for row in ds:\n            # Initialize lists for storing predictions and offsets\n            row_preds = []\n            row_offset = []\n\n            # Iterate over tokens and their offset mappings in the row\n            for i, y in enumerate(row[\"offset_mapping\"]):\n                # Create a new dataset for each split of the document\n                x = Dataset.from_dict({\n                    \"token_type_ids\":[row[\"token_type_ids\"][i]],\n                    \"input_ids\":[row[\"input_ids\"][i]],\n                    \"attention_mask\":[row[\"attention_mask\"][i]],\n                    \"offset_mapping\":[row[\"offset_mapping\"][i]]\n                })\n                # Predict for the split\n                pred = trainer.predict(x).predictions\n                # Remove stride and additional CLS & SEP tokens\n                row_preds.append(self.backwards_map_preds(pred, len(row[\"offset_mapping\"]), CFG))\n                row_offset += self.backwards_map_(y, len(row[\"offset_mapping\"]), CFG)\n\n            # Finalize row\n            ds_dict[\"document\"].append(row[\"document\"])\n            ds_dict[\"tokens\"].append(row[\"tokens\"])\n            ds_dict[\"token_map\"].append(row[\"token_map\"])\n            ds_dict[\"offset_mapping\"].append(row_offset)\n            \n            # Finalize prediction collection by concatenating\n            p_concat = np.concatenate(row_preds, axis = 1)\n            preds.append(p_concat)\n\n        # Load model configuration\n        config = json.load(open(Path(CFG.checkpoint) / \"config.json\"))\n        id2label = config[\"id2label\"]\n\n        # Finalize predictions\n        preds_final = []\n        for predictions in preds:\n            predictions_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n            predictions = predictions.argmax(-1)\n            predictions_without_O = predictions_softmax[:,:,:12].argmax(-1)\n            O_predictions = predictions_softmax[:,:,12]\n\n            threshold = CFG.threshold\n            preds_final.append(np.where(O_predictions < threshold, predictions_without_O , predictions))\n\n        # Create a new dataset from the dictionary\n        ds = Dataset.from_dict(ds_dict)\n\n        # Initialize empty lists for storing pairs\n        pairs = []\n        document, token, label, token_str = [], [], [], []\n\n        # Iterate over predictions, token maps, offsets, tokens, and documents\n        for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n            # Iterate over each token prediction and its offset\n            for token_pred, (start_idx, end_idx) in zip(p[0], offsets):\n                label_pred = id2label[str(token_pred)]\n\n                if start_idx + end_idx == 0: continue\n\n                if token_map[start_idx] == -1:\n                    start_idx += 1\n\n                # Ignore \"\\n\\n\"\n                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                    start_idx += 1\n\n                if start_idx >= len(token_map): break\n\n                token_id = token_map[start_idx]\n\n                # Ignore \"O\" predictions and whitespace predictions\n                if label_pred != \"O\" and token_id != -1:\n                    pair=(doc, token_id)\n\n                    if pair not in pairs:\n                        document.append(doc)\n                        token.append(token_id)\n                        label.append(label_pred)\n                        token_str.append(tokens[token_id])\n                        pairs.append(pair)\n\n        # Create DataFrame\n        df = pd.DataFrame({\n            \"document\": document,\n            \"token\": token,\n            \"label\": label,\n            \"token_str\": token_str\n        })\n        df[\"row_id\"] = list(range(len(df)))\n\n        return df\n    \n# Initialize ModelInference class\nmi = ModelInference()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:32:50.982091Z","iopub.execute_input":"2024-05-13T11:32:50.982443Z","iopub.status.idle":"2024-05-13T11:32:51.007903Z","shell.execute_reply.started":"2024-05-13T11:32:50.982405Z","shell.execute_reply":"2024-05-13T11:32:51.006656Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Process dataset and create DataFrame\ndf = mi.process_dataset(ds, tokenizer, CFG)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:32:51.009118Z","iopub.execute_input":"2024-05-13T11:32:51.009470Z","iopub.status.idle":"2024-05-13T11:33:34.390800Z","shell.execute_reply.started":"2024-05-13T11:32:51.009438Z","shell.execute_reply":"2024-05-13T11:33:34.389180Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"# Display DataFrame\ndisplay(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:33:34.393256Z","iopub.execute_input":"2024-05-13T11:33:34.393664Z","iopub.status.idle":"2024-05-13T11:33:34.407694Z","shell.execute_reply.started":"2024-05-13T11:33:34.393630Z","shell.execute_reply":"2024-05-13T11:33:34.406575Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"    document  token           label   token_str  row_id\n0          7      9  B-NAME_STUDENT    Nathalie       0\n1          7     10  I-NAME_STUDENT       Sylla       1\n2          7    482  B-NAME_STUDENT    Nathalie       2\n3          7    483  I-NAME_STUDENT       Sylla       3\n4          7    741  B-NAME_STUDENT    Nathalie       4\n5          7    742  I-NAME_STUDENT       Sylla       5\n6         10      0  B-NAME_STUDENT       Diego       6\n7         10      1  I-NAME_STUDENT     Estrada       7\n8         10    464  B-NAME_STUDENT       Diego       8\n9         10    465  I-NAME_STUDENT     Estrada       9\n10        16      4  B-NAME_STUDENT    Gilberto      10\n11        16      5  I-NAME_STUDENT      Gamboa      11\n12        86      6  B-NAME_STUDENT      Eladio      12\n13        86      7  I-NAME_STUDENT       Amaya      13\n14        93      0  B-NAME_STUDENT      Silvia      14\n15        93      1  I-NAME_STUDENT  Villalobos      15\n16       104      8  B-NAME_STUDENT       Sakir      16\n17       104      9  I-NAME_STUDENT       Ahmad      17\n18       112      5  B-NAME_STUDENT   Francisco      18\n19       112      6  I-NAME_STUDENT    Ferreira      19","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"comp = pd.read_csv(\"/kaggle/input/pii-detection-deberta/output.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:34:28.040842Z","iopub.execute_input":"2024-05-13T11:34:28.041291Z","iopub.status.idle":"2024-05-13T11:34:28.056644Z","shell.execute_reply.started":"2024-05-13T11:34:28.041257Z","shell.execute_reply":"2024-05-13T11:34:28.054889Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"comp","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:34:29.596366Z","iopub.execute_input":"2024-05-13T11:34:29.596861Z","iopub.status.idle":"2024-05-13T11:34:29.609396Z","shell.execute_reply.started":"2024-05-13T11:34:29.596822Z","shell.execute_reply":"2024-05-13T11:34:29.607844Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"    Token Position           label\n0                9  B-NAME_STUDENT\n1               10  I-NAME_STUDENT\n2              482  B-NAME_STUDENT\n3              483  I-NAME_STUDENT\n4              741  B-NAME_STUDENT\n5              742  I-NAME_STUDENT\n6                0  B-NAME_STUDENT\n7                1  I-NAME_STUDENT\n8              464  B-NAME_STUDENT\n9              465  I-NAME_STUDENT\n10               4  B-NAME_STUDENT\n11               5  I-NAME_STUDENT\n12               5  B-NAME_STUDENT\n13               6  I-NAME_STUDENT\n14              12  B-NAME_STUDENT\n15              13  I-NAME_STUDENT\n16               6  B-NAME_STUDENT\n17               7  I-NAME_STUDENT\n18               0  B-NAME_STUDENT\n19               1  I-NAME_STUDENT\n20               8  B-NAME_STUDENT\n21               9  I-NAME_STUDENT\n22               5  B-NAME_STUDENT\n23               6  I-NAME_STUDENT\n24              32  B-NAME_STUDENT\n25              33  I-NAME_STUDENT","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Token Position</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ntotal_rows_df = len(df)\ntotal_rows_comp = len(comp)\n\n# Initialize a counter for matching rows\nmatching_rows = 0\n\n# Iterate through rows in df\nfor index, row in df.iterrows():\n    token_df = row['token']\n    label_df = row['label']\n    \n    # Iterate through rows in comp\n    for index_comp, row_comp in comp.iterrows():\n        token_comp = row_comp['Token Position']\n        label_comp = row_comp['label']\n        \n        # Compare 'Token' and 'Label' values\n        if token_df == token_comp and label_df == label_comp:\n            matching_rows += 1\n            break  # Exit the inner loop if a match is found\n\n# Calculate accuracy\naccuracy = (matching_rows / total_rows_df) * 100\n\nprint(f\"Accuracy: {accuracy:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:34:45.635412Z","iopub.execute_input":"2024-05-13T11:34:45.635849Z","iopub.status.idle":"2024-05-13T11:34:45.665662Z","shell.execute_reply.started":"2024-05-13T11:34:45.635817Z","shell.execute_reply":"2024-05-13T11:34:45.664127Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Accuracy: 100.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df and comp are your DataFrames\n# Initialize variables for TP, FP, and FN\nTP = 0\nFP = 0\nFN = 0\n\n# Iterate through rows in df\nfor index, row in df.iterrows():\n    token_df = row['token']\n    label_df = row['label']\n    \n    # Initialize a flag for matching\n    match_found = False\n    \n    # Iterate through rows in comp\n    for index_comp, row_comp in comp.iterrows():\n        token_comp = row_comp['Token Position']\n        label_comp = row_comp['label']\n        \n        # Compare 'Token' and 'Label' values\n        if token_df == token_comp and label_df == label_comp:\n            match_found = True\n            break  # Exit the inner loop if a match is found\n    \n    # Update TP, FP, and FN counts\n    if match_found:\n        TP += 1\n    else:\n        FN += 1\n\n# Calculate FP as the difference between total rows in comp and TP\nFP = total_rows_comp - TP\n\n# Calculate precision and recall\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nFbetaScore=26*(precision*recall)/(25*precision+recall)\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"FbetaScore: {FbetaScore:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:43:15.986166Z","iopub.execute_input":"2024-05-13T11:43:15.986548Z","iopub.status.idle":"2024-05-13T11:43:16.015046Z","shell.execute_reply.started":"2024-05-13T11:43:15.986519Z","shell.execute_reply":"2024-05-13T11:43:16.013346Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Precision: 0.77\nRecall: 1.00\nFbetaScore: 0.99\n","output_type":"stream"}]},{"cell_type":"code","source":"df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T11:39:34.949126Z","iopub.execute_input":"2024-05-13T11:39:34.949805Z","iopub.status.idle":"2024-05-13T11:39:34.969478Z","shell.execute_reply.started":"2024-05-13T11:39:34.949715Z","shell.execute_reply":"2024-05-13T11:39:34.968185Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}