{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 66653,
          "databundleVersionId": 7500999,
          "sourceType": "competition"
        },
        {
          "sourceId": 7718665,
          "sourceType": "datasetVersion",
          "datasetId": 4508259
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:11:38.593829Z",
          "iopub.execute_input": "2024-05-12T23:11:38.594223Z",
          "iopub.status.idle": "2024-05-12T23:11:54.468115Z",
          "shell.execute_reply.started": "2024-05-12T23:11:38.594188Z",
          "shell.execute_reply": "2024-05-12T23:11:54.467025Z"
        },
        "trusted": true,
        "id": "hFcFP5ZENF7d",
        "outputId": "0174e45b-25cd-4765-a0be-cd184448a4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting faker\n  Downloading Faker-25.1.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: python-dateutil>=2.4 in /opt/conda/lib/python3.10/site-packages (from faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\nDownloading Faker-25.1.0-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: faker\nSuccessfully installed faker-25.1.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:22:18.638846Z",
          "iopub.execute_input": "2024-05-12T23:22:18.639244Z",
          "iopub.status.idle": "2024-05-12T23:22:32.044303Z",
          "shell.execute_reply.started": "2024-05-12T23:22:18.639212Z",
          "shell.execute_reply": "2024-05-12T23:22:32.043136Z"
        },
        "trusted": true,
        "id": "CwwixVyLNF7e",
        "outputId": "52a91ba8-c510-44df-a58b-6af19477fee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting openai\n  Downloading openai-1.28.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\nDownloading openai-1.28.1-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: openai\nSuccessfully installed openai-1.28.1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "train = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-12T23:38:18.098151Z",
          "iopub.execute_input": "2024-05-12T23:38:18.098519Z",
          "iopub.status.idle": "2024-05-12T23:38:20.889935Z",
          "shell.execute_reply.started": "2024-05-12T23:38:18.098491Z",
          "shell.execute_reply": "2024-05-12T23:38:20.888927Z"
        },
        "trusted": true,
        "id": "9PDOfh6MNF7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:49:39.873158Z",
          "iopub.execute_input": "2024-05-12T23:49:39.873561Z",
          "iopub.status.idle": "2024-05-12T23:49:39.881376Z",
          "shell.execute_reply.started": "2024-05-12T23:49:39.873529Z",
          "shell.execute_reply": "2024-05-12T23:49:39.879929Z"
        },
        "trusted": true,
        "id": "2jMEx4rBNF7e",
        "outputId": "f7aee9b6-0e18-4da9-f808-d80e19b4a6e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "list"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:40:05.513601Z",
          "iopub.execute_input": "2024-05-12T23:40:05.514154Z",
          "iopub.status.idle": "2024-05-12T23:40:05.541267Z",
          "shell.execute_reply.started": "2024-05-12T23:40:05.514117Z",
          "shell.execute_reply": "2024-05-12T23:40:05.540047Z"
        },
        "trusted": true,
        "id": "EEyGKBptNF7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_documents_to_dict_with_sets(documents):\n",
        "    all_entities_dicts = []\n",
        "\n",
        "    for document in documents:\n",
        "        tokens = document[\"tokens\"]\n",
        "        labels = document[\"labels\"]\n",
        "        entities_dict = {}\n",
        "        current_entity = \"\"\n",
        "        current_label = \"\"\n",
        "\n",
        "        for token, label in zip(tokens, labels):\n",
        "            if label.startswith(\"B-\"):\n",
        "                if current_entity:  # Save the previous entity if it exists\n",
        "                    entities_dict.setdefault(current_label, set()).add(current_entity)\n",
        "                current_entity = token  # Start a new entity\n",
        "                current_label = label[2:]  # Extract entity type without the B-\n",
        "            elif label.startswith(\"I-\") and label[2:] == current_label:\n",
        "                current_entity += \" \" + token  # Continue building the entity\n",
        "            else:  # Either an 'O' label or a different entity type\n",
        "                if current_entity:  # Save the previous entity if it exists\n",
        "                    entities_dict.setdefault(current_label, set()).add(current_entity)\n",
        "                    current_entity = \"\"\n",
        "                    current_label = \"\"\n",
        "\n",
        "        # Check if there's an entity still being built\n",
        "        if current_entity:\n",
        "            entities_dict.setdefault(current_label, set()).add(current_entity)\n",
        "\n",
        "        all_entities_dicts.append((entities_dict, set([l.split(\"-\")[-1] for l in labels]), document[\"full_text\"]))\n",
        "\n",
        "    return all_entities_dicts"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:11:03.917466Z",
          "iopub.execute_input": "2024-05-12T23:11:03.917827Z",
          "iopub.status.idle": "2024-05-12T23:11:03.930535Z",
          "shell.execute_reply.started": "2024-05-12T23:11:03.917796Z",
          "shell.execute_reply": "2024-05-12T23:11:03.929077Z"
        },
        "trusted": true,
        "id": "0Y2TqAz4NF7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"Rephrase the following essay in different way while substituting following PIIs with the given ones,\\n {}\\n\\n-------------------------------------\\n{}\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:11:07.375795Z",
          "iopub.execute_input": "2024-05-12T23:11:07.376142Z",
          "iopub.status.idle": "2024-05-12T23:11:07.380723Z",
          "shell.execute_reply.started": "2024-05-12T23:11:07.376115Z",
          "shell.execute_reply": "2024-05-12T23:11:07.379532Z"
        },
        "trusted": true,
        "id": "F0LixBfANF7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "fake = Faker()\n",
        "common_email_domains = [\n",
        "    \"gmail.com\",\n",
        "    \"yahoo.com\",\n",
        "    \"outlook.com\",\n",
        "    \"icloud.com\",\n",
        "    \"aol.com\",\n",
        "    \"mail.com\",\n",
        "    \"yandex.com\",\n",
        "    \"protonmail.com\",\n",
        "    \"zoho.com\"\n",
        "]\n",
        "\n",
        "def generate_random_id():\n",
        "    patterns = [\n",
        "        lambda: str(random.randint(100000000000, 999999999999)),  # 12-digit numbers\n",
        "        lambda: f\"{''.join([random.choice(string.ascii_uppercase+string.ascii_lowercase) for i in range(2)]).title()}:{random.randint(100000000000, 999999999999)}\",  # Prefix:numbers\n",
        "        lambda: ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)),  # Alphanumeric (8 characters)\n",
        "        lambda: f\"{''.join(random.choices(string.ascii_uppercase +string.ascii_lowercase + string.digits, k=8))}_{random.randint(1000000000, 9999999999)}\",  # Unique structures\n",
        "        lambda: ''.join(random.choices(string.ascii_uppercase + string.digits, k=random.randint(5, 10))),  # Variable length Alphanumeric\n",
        "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(10000, 99999)}\",  # Single letter followed by 5 digits\n",
        "        lambda: \",\".join([str(random.randint(0, 99)) for _ in range(4)]),  # Comma-separated numbers\n",
        "    ]\n",
        "\n",
        "    # Randomly select and call a pattern function\n",
        "    return random.choice(patterns)()\n",
        "\n",
        "\n",
        "def generate_random_url():\n",
        "    # Base URLs for pattern matching\n",
        "    base_urls = [\n",
        "        \"https://www.youtube.com/watch?v=\",\n",
        "        \"http://www.\",\n",
        "        \"https://\",\n",
        "        \"http://\",\n",
        "        \"https://youtu.be/\",\n",
        "        \"https://www.linkedin.com/in/\",\n",
        "        \"https://www.facebook.com/\"\n",
        "    ]\n",
        "\n",
        "    # Paths and filenames for pattern matching\n",
        "    paths = [\n",
        "        \"/wp-content/main/searchregister.htm\",\n",
        "        \"/postsabout.html\",\n",
        "        \"/category/tags/wp-contentfaq.htm\",\n",
        "        \"/tagsmain.html\",\n",
        "        \"/explorelogin.htm\",\n",
        "        \"/categories/search/tagsmain.html\",\n",
        "        \"/main/exploreregister.jsp\",\n",
        "        \"/wp-content/appcategory.php\",\n",
        "        \"/tag/app/listmain.php\",\n",
        "        \"/app/tags/searchregister.php\",\n",
        "        \"/tagspost.asp\",\n",
        "        \"/main/list/postshomepage.html\",\n",
        "        \"/categories/wp-content/tagabout.html\",\n",
        "        \"/explore/tagterms.php\",\n",
        "        \"/list/appindex.html\",\n",
        "        \"/app/tagsauthor.html\",\n",
        "        \"/listregister.asp\",\n",
        "        \"/categories/appabout.asp\",\n",
        "        \"/wp-contenthome.html\",\n",
        "        \"/search/categories/wp-contentlogin.htm\",\n",
        "        \"/tags/listfaq.html\",\n",
        "        \"/wp-content/categorieshomepage.html\",\n",
        "        \"/tagmain.html\",\n",
        "        \"/appcategory.html\",\n",
        "        \"/bloghome.jsp\",\n",
        "        \"/categoriesindex.html\",\n",
        "        \"/search/wp-content/categorypost.jsp\",\n",
        "        \"/posts/wp-content/categoryauthor.php\",\n",
        "        \"/tag/searchauthor.html\",\n",
        "        \"/listpost.html\",\n",
        "        \"/tags/wp-contentpost.html\",\n",
        "        \"/blog/search/searchprivacy.php\",\n",
        "        \"/list/taghome.php\",\n",
        "        \"/wp-content/mainprivacy.htm\",\n",
        "        \"/list/explorehomepage.htm\",\n",
        "        \"/blog/main/appfaq.jsp\",\n",
        "        \"/wp-contentcategory.php\",\n",
        "        \"/tags/explore/blogpost.php\",\n",
        "        \"/app/categoriesindex.php\",\n",
        "        \"/search/tags/tagsearch.php\",\n",
        "        \"/posts/searchauthor.php\",\n",
        "        \"/mainlogin.htm\",\n",
        "        \"/tag/tags/categoriesprivacy.htm\",\n",
        "        \"/postsindex.jsp\",\n",
        "        \"/wp-content/app/categorieshomepage.jsp\",\n",
        "        \"/tagssearch.php\",\n",
        "        \"/posts/tagspost.htm\",\n",
        "        \"/list/taghomepage.htm\",\n",
        "        \"/main/tag/listindex.htm\",\n",
        "        \"/blogprivacy.asp\",\n",
        "        \"/exploreprivacy.html\",\n",
        "        \"/exploreabout.jsp\",\n",
        "        \"/wp-content/wp-contentpost.html\",\n",
        "        \"/main/posts/tagprivacy.asp\",\n",
        "        \"/wp-content/category/bloghome.php\",\n",
        "        \"/list/postshomepage.jsp\",\n",
        "        \"/posts/blog/searchabout.html\",\n",
        "        \"/exploremain.html\",\n",
        "        \"/blogsearch.html\",\n",
        "        \"/tag/blogindex.asp\",\n",
        "        \"/category/wp-contentregister.htm\",\n",
        "        \"/categorycategory.jsp\",\n",
        "        \"/list/exploreauthor.html\",\n",
        "        \"/category/tagprivacy.php\",\n",
        "        \"/tag/wp-contentlogin.htm\",\n",
        "        \"/categories/search/tagprivacy.htm\"\n",
        "    ]\n",
        "\n",
        "    # Randomly choose a base URL and a path\n",
        "    base_url = random.choice(base_urls)\n",
        "    path = random.choice(paths)\n",
        "\n",
        "    # Generate a random string for YouTube video IDs or domain names\n",
        "    if \"youtube\" in base_url or \"youtu.be\" in base_url:\n",
        "        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=11))\n",
        "    else:\n",
        "        domain = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 10)))\n",
        "        domain_extension = random.choice([\".com\", \".biz\", \".info\", \".net\", \".org\"])\n",
        "        random_string = domain + domain_extension\n",
        "\n",
        "    # Combine elements to form the URL\n",
        "    if \"linkedin.com/in/\" in base_url or \"facebook.com/\" in base_url:\n",
        "        name = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 10)))\n",
        "        random_url = base_url + name\n",
        "    elif \"youtube\" in base_url or \"youtu.be\" in base_url:\n",
        "        random_url = base_url + random_string\n",
        "    else:\n",
        "        random_url = base_url + random_string + path\n",
        "\n",
        "    return random_url\n",
        "\n",
        "\n",
        "\n",
        "def generate_fake_pii(pii_class):\n",
        "    if pii_class == 'EMAIL':\n",
        "        return fake.email().split(\"@\")[0]+\"@\"+random.choice(common_email_domains)\n",
        "    elif pii_class == 'ID_NUM':\n",
        "        # Simulate an ID number with a random number; adjust format as needed\n",
        "        return str(generate_random_id())\n",
        "    elif pii_class == 'NAME_STUDENT':\n",
        "        return fake.name()\n",
        "    elif pii_class == 'PHONE_NUM':\n",
        "        return fake.phone_number().replace(\"-\",\" - \").replace(\"(\",\"( \")\n",
        "    elif pii_class == 'STREET_ADDRESS':\n",
        "        return fake.address().replace(\"\\n\", \"  \\n\")\n",
        "    elif pii_class == 'URL_PERSONAL':\n",
        "        # Generate a simple personal URL\n",
        "        return generate_random_url()\n",
        "    elif pii_class == 'USERNAME':\n",
        "        return fake.user_name()\n",
        "    else:\n",
        "        raise\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:12:07.137945Z",
          "iopub.execute_input": "2024-05-12T23:12:07.138353Z",
          "iopub.status.idle": "2024-05-12T23:12:07.264564Z",
          "shell.execute_reply.started": "2024-05-12T23:12:07.138319Z",
          "shell.execute_reply": "2024-05-12T23:12:07.263238Z"
        },
        "trusted": true,
        "id": "CJv2jfjANF7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "  # Process the sentence\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize our dictionary with empty lists for tokens, locations, and trailing whitespace flags\n",
        "    token_details = {\"tokens\": [], \"locations\": [], \"trailing_whitespace\": []}\n",
        "\n",
        "    # Iterate through the tokens in the document\n",
        "    for token in doc:\n",
        "        # Add the token text to the 'tokens' list\n",
        "        token_details[\"tokens\"].append(token.text)\n",
        "\n",
        "        # Calculate and add the start and end indices (including trailing space if present)\n",
        "        start, end = token.idx, token.idx + len(token.text)\n",
        "        if token.whitespace_:\n",
        "            end += 1\n",
        "        token_details[\"locations\"].append((start, end))\n",
        "\n",
        "        # Add a boolean flag indicating the presence of trailing whitespace\n",
        "        token_details[\"trailing_whitespace\"].append(bool(token.whitespace_))\n",
        "\n",
        "    # Printing the structured dictionary\n",
        "    return token_details\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:12:23.179901Z",
          "iopub.execute_input": "2024-05-12T23:12:23.180275Z",
          "iopub.status.idle": "2024-05-12T23:12:29.294442Z",
          "shell.execute_reply.started": "2024-05-12T23:12:23.180245Z",
          "shell.execute_reply": "2024-05-12T23:12:29.292974Z"
        },
        "trusted": true,
        "id": "ubhCczIANF7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_substring_locations(large_string, substring):\n",
        "    start = 0\n",
        "    locations = []\n",
        "    while True:\n",
        "        # Find next index of substring, starting search at `start`\n",
        "        index = large_string.find(substring, start)\n",
        "        if index == -1:  # If no more occurrences are found, break the loop\n",
        "            break\n",
        "        locations.append((index, index+len(substring)))  # Add the found index to the list\n",
        "        start = index + 1  # Move the start to the next position for the next search\n",
        "    return locations\n",
        "\n",
        "def allign_labels(token_details, entities, essay):\n",
        "    l = len(token_details[\"tokens\"])\n",
        "    labels = ['O']*l\n",
        "    full_text = \"\"\n",
        "    for i in range(len(token_details[\"tokens\"])):\n",
        "        if token_details[\"trailing_whitespace\"][i]:\n",
        "            full_text+=token_details[\"tokens\"][i]+\" \"\n",
        "        else:\n",
        "            full_text+=token_details[\"tokens\"][i]\n",
        "\n",
        "    for e in entities:\n",
        "        if entities[e]:\n",
        "            #print(e)\n",
        "            if e == 'NAME_STUDENT':\n",
        "                pieces = entities[e][0].split()\n",
        "                for j in range(len(pieces)):\n",
        "                    for i in range(len(token_details[\"tokens\"])):\n",
        "                        if token_details[\"tokens\"][i] == pieces[j].strip():\n",
        "                            labels[i] = ['B-', 'I-'][j!=0]+e\n",
        "            elif e == 'STREET_ADDRESS':\n",
        "                format_1 = entities[e][0]\n",
        "                format_2 = entities[e][0].replace(\"\\n\", \" \\n\")\n",
        "                format_3 = entities[e][0].replace(\"\\n\", \"  \\n\")\n",
        "                format_4 = entities[e][0].replace(\"  \\n\", \"\\n\")\n",
        "                locations_1 = find_substring_locations(full_text, format_1)\n",
        "                locations_2 = find_substring_locations(full_text, format_2)\n",
        "                locations_3 = find_substring_locations(full_text, format_3)\n",
        "                locations_4 = find_substring_locations(full_text, format_4)\n",
        "                locations = set(locations_1 + locations_2 + locations_3+locations_4)\n",
        "\n",
        "                for location in locations:\n",
        "                    first=False\n",
        "                    for i in range(len(token_details[\"tokens\"])):\n",
        "                        if token_details[\"locations\"][i][1] > location[0] and token_details[\"locations\"][i][0] < location[1]:\n",
        "                            if not first:\n",
        "                                labels[i] = 'B-'+e\n",
        "                                first = True\n",
        "                            else:\n",
        "                                labels[i] = 'I-'+e\n",
        "            else:\n",
        "                locations = find_substring_locations(full_text, entities[e][0])\n",
        "                #print(e, locations)\n",
        "                for location in locations:\n",
        "                    first=False\n",
        "                    for i in range(len(token_details[\"tokens\"])):\n",
        "                        if token_details[\"locations\"][i][1] > location[0] and token_details[\"locations\"][i][0] < location[1]:\n",
        "                            if not first:\n",
        "                                labels[i] = 'B-'+e\n",
        "                                first = True\n",
        "                            else:\n",
        "                                labels[i] = 'I-'+e\n",
        "\n",
        "    return labels\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:21:41.997403Z",
          "iopub.execute_input": "2024-05-12T23:21:41.998433Z",
          "iopub.status.idle": "2024-05-12T23:21:42.025827Z",
          "shell.execute_reply.started": "2024-05-12T23:21:41.998383Z",
          "shell.execute_reply": "2024-05-12T23:21:42.024175Z"
        },
        "trusted": true,
        "id": "O9F8I3CgNF7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "api_key = '<api-key>'\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def generate_response(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=False,\n",
        "        temperature=0.7,\n",
        "        max_tokens = 4000\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:41:36.389588Z",
          "iopub.execute_input": "2024-05-12T23:41:36.389999Z",
          "iopub.status.idle": "2024-05-12T23:41:36.411262Z",
          "shell.execute_reply.started": "2024-05-12T23:41:36.389968Z",
          "shell.execute_reply": "2024-05-12T23:41:36.409799Z"
        },
        "trusted": true,
        "id": "lfMzhk50NF7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "documents = train\n",
        "full_entities_dict = process_documents_to_dict_with_sets(documents)\n",
        "try:\n",
        "    with open(\"./generated_raw_data.json\", \"r\") as fp:\n",
        "        results = json.load(fp)\n",
        "except:\n",
        "    results = []\n",
        "for c in range(10):\n",
        "    for entities, _, full_text in tqdm(full_entities_dict):\n",
        "        piis=\"\"\n",
        "        generated_entities = {\n",
        "          \"EMAIL\":[],\n",
        "          \"ID_NUM\":[],\n",
        "          \"USERNAME\":[],\n",
        "          \"PHONE_NUM\":[],\n",
        "          \"STREET_ADDRESS\":[],\n",
        "          \"URL_PERSONAL\":[],\n",
        "          \"NAME_STUDENT\":[]\n",
        "        }\n",
        "        for k in entities.keys():\n",
        "            generated_entity = generate_fake_pii(k)\n",
        "            generated_entities[k].append(generated_entity)\n",
        "            piis+=list(entities[k])[-1]+\"({}) => {}\".format(k, generated_entity)+\",\\n\"\n",
        "        prompt = prompt_text.format(piis.strip(\"\\n,\"), full_text)\n",
        "        essay = generate_response(prompt)\n",
        "        result = {\n",
        "          \"prompt\":prompt,\n",
        "          \"essay\":essay,\n",
        "          \"entities\":generated_entities\n",
        "        }\n",
        "        results.append(result)\n",
        "        with open(\"./generated_raw_data.json\", \"w\") as fp:\n",
        "            json.dump(results, fp)\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "    final_documents = []\n",
        "    for result in tqdm(results):\n",
        "        token_details = tokenize(result[\"essay\"])\n",
        "        labels = allign_labels(token_details, result[\"entities\"], result[\"essay\"])\n",
        "        final_documents.append({\"tokens\": token_details[\"tokens\"], \"trailing_whitespace\": token_details[\"trailing_whitespace\"], \"labels\": labels})\n",
        "\n",
        "    with open(\"./generated_tokenized_data.json\", \"w\") as fp:\n",
        "        json.dump(final_documents, fp)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-12T23:50:05.330897Z",
          "iopub.execute_input": "2024-05-12T23:50:05.331281Z",
          "iopub.status.idle": "2024-05-13T10:54:08.589985Z",
          "shell.execute_reply.started": "2024-05-12T23:50:05.331253Z",
          "shell.execute_reply": "2024-05-13T10:54:08.587818Z"
        },
        "trusted": true,
        "id": "CTYHjt7SNF7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}