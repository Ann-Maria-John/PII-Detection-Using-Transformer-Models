{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8371121,"sourceType":"datasetVersion","datasetId":4976598},{"sourceId":168532349,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":33.027039,"end_time":"2024-03-24T08:41:45.176461","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-24T08:41:12.149422","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{}},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# Handle warnings\n","import warnings\n","warnings.simplefilter('ignore')"],"metadata":{"papermill":{"duration":0.026919,"end_time":"2024-03-24T08:41:14.771732","exception":false,"start_time":"2024-03-24T08:41:14.744813","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:35.171960Z","iopub.execute_input":"2024-05-10T02:38:35.172349Z","iopub.status.idle":"2024-05-10T02:38:35.184299Z","shell.execute_reply.started":"2024-05-10T02:38:35.172317Z","shell.execute_reply":"2024-05-10T02:38:35.183287Z"},"trusted":true,"id":"3rIYPoojsS8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Workflow essentials\n","import gc\n","import json\n","import torch\n","import random\n","import argparse\n","from pathlib import Path\n","from itertools import chain\n","from functools import partial\n","from datasets import Dataset, features\n","\n","# Data preprocessing and visualization\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Model development\n","from sklearn.metrics import f1_score\n","from transformers import AutoModel, AutoTokenizer\n","from transformers import Trainer, TrainingArguments\n","from transformers import AutoModelForTokenClassification\n","from transformers import DataCollatorForTokenClassification"],"metadata":{"_kg_hide-output":true,"papermill":{"duration":17.728127,"end_time":"2024-03-24T08:41:32.512613","exception":false,"start_time":"2024-03-24T08:41:14.784486","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:35.203613Z","iopub.execute_input":"2024-05-10T02:38:35.203941Z","iopub.status.idle":"2024-05-10T02:38:56.523762Z","shell.execute_reply.started":"2024-05-10T02:38:35.203912Z","shell.execute_reply":"2024-05-10T02:38:56.522604Z"},"trusted":true,"id":"7KY79yr9sS8M","outputId":"2f12ca5c-cdf3-48fb-fbd0-5e4d5504a61e"},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-05-10 02:38:47.673038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-10 02:38:47.673169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-10 02:38:47.805558: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":["class CFG:\n","    # Path to test dataset\n","    test_data = '/kaggle/input/pii-detection-removal-from-educational-data/test.json'\n","\n","    # Path to saved model checkpoint\n","    checkpoint = '/kaggle/input/piidd-train-deberta-with-hugging-face/deberta3base'\n","\n","    # Data preprocessing\n","    max_len = 3072\n","    workers = 4\n","\n","    # Model inference\n","    threshold = 0.84\n","    batch_size = 4\n","    stride = 256\n","    seed = 457"],"metadata":{"papermill":{"duration":0.020967,"end_time":"2024-03-24T08:41:32.546791","exception":false,"start_time":"2024-03-24T08:41:32.525824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:56.525865Z","iopub.execute_input":"2024-05-10T02:38:56.527018Z","iopub.status.idle":"2024-05-10T02:38:56.532524Z","shell.execute_reply.started":"2024-05-10T02:38:56.526979Z","shell.execute_reply":"2024-05-10T02:38:56.531445Z"},"trusted":true,"id":"vcDNBN5KsS8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def global_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)"],"metadata":{"papermill":{"duration":0.019874,"end_time":"2024-03-24T08:41:32.579515","exception":false,"start_time":"2024-03-24T08:41:32.559641","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:56.533673Z","iopub.execute_input":"2024-05-10T02:38:56.533984Z","iopub.status.idle":"2024-05-10T02:38:56.545411Z","shell.execute_reply.started":"2024-05-10T02:38:56.533949Z","shell.execute_reply":"2024-05-10T02:38:56.544524Z"},"trusted":true,"id":"TNue0bb5sS8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set seed for reproducibility across multiple libraries\n","global_seed(CFG.seed)"],"metadata":{"papermill":{"duration":0.021385,"end_time":"2024-03-24T08:41:32.614707","exception":false,"start_time":"2024-03-24T08:41:32.593322","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:56.548072Z","iopub.execute_input":"2024-05-10T02:38:56.548401Z","iopub.status.idle":"2024-05-10T02:38:56.557888Z","shell.execute_reply.started":"2024-05-10T02:38:56.548375Z","shell.execute_reply":"2024-05-10T02:38:56.556882Z"},"trusted":true,"id":"zYbg451_sS8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gc.collect()"],"metadata":{"papermill":{"duration":0.292228,"end_time":"2024-03-24T08:41:32.919752","exception":false,"start_time":"2024-03-24T08:41:32.627524","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:56.559373Z","iopub.execute_input":"2024-05-10T02:38:56.559793Z","iopub.status.idle":"2024-05-10T02:38:56.859572Z","shell.execute_reply.started":"2024-05-10T02:38:56.559759Z","shell.execute_reply":"2024-05-10T02:38:56.858553Z"},"trusted":true,"id":"4S467iX6sS8O","outputId":"3fe960ac-7ca9-435d-839d-d35aa188f789"},"execution_count":null,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"code","source":["class DataPreprocessor:\n","    def tokenize(self, example, tokenizer):\n","        # Initialize lists to store tokenized text and token map\n","        text, token_map = [], []\n","\n","        # Initialize index for token mapping\n","        idx = 0\n","\n","        # Iterate over tokens and trailing whitespace in the example\n","        for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            # Append token to the text list\n","            text.append(t)\n","\n","            # Extend token map with index for the current token repeated for its length\n","            token_map.extend([idx] * len(t))\n","\n","            # If there is trailing whitespace after the token, append a space to the text and mark its position as -1 in the token map\n","            if ws:\n","                text.append(\" \")\n","                token_map.append(-1)\n","\n","            # Increment the index for token mapping\n","            idx += 1\n","\n","        # Tokenize the text using the provided tokenizer\n","        tokenized = tokenizer(\"\".join(text),\n","                              return_offsets_mapping=True,\n","                              truncation=True,\n","                              max_length=CFG.max_len,\n","                              stride=CFG.stride,\n","                              return_overflowing_tokens=True)\n","\n","        # Return the tokenized text along with the token map\n","        return {\n","            **tokenized,\n","            \"token_map\": token_map,\n","        }\n","\n","# Initialize DataPreprocessor class\n","dp = DataPreprocessor()"],"metadata":{"papermill":{"duration":0.02356,"end_time":"2024-03-24T08:41:32.983245","exception":false,"start_time":"2024-03-24T08:41:32.959685","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:56.861178Z","iopub.execute_input":"2024-05-10T02:38:56.861502Z","iopub.status.idle":"2024-05-10T02:38:56.871291Z","shell.execute_reply.started":"2024-05-10T02:38:56.861475Z","shell.execute_reply":"2024-05-10T02:38:56.870252Z"},"trusted":true,"id":"eyK9rZFcsS8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load test data\n","data = json.load(open(CFG.test_data))\n","\n","# Create the dataset\n","ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [x[\"document\"] for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","})\n","\n","# Initialize the tokenizer for the pre-trained model checkpoint\n","tokenizer = AutoTokenizer.from_pretrained(CFG.checkpoint)\n","\n","# Tokenize the test essays\n","ds = ds.map(dp.tokenize,\n","            fn_kwargs={\"tokenizer\": tokenizer},\n","            num_proc=CFG.workers)"],"metadata":{"_kg_hide-output":true,"papermill":{"duration":1.598589,"end_time":"2024-03-24T08:41:34.594744","exception":false,"start_time":"2024-03-24T08:41:32.996155","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:56.872644Z","iopub.execute_input":"2024-05-10T02:38:56.872961Z","iopub.status.idle":"2024-05-10T02:38:58.746286Z","shell.execute_reply.started":"2024-05-10T02:38:56.872936Z","shell.execute_reply":"2024-05-10T02:38:58.745084Z"},"trusted":true,"id":"X7raKUD4sS8O","outputId":"56907efb-fbdf-4e04-dd4b-491795f9cf9c","colab":{"referenced_widgets":["d0e9ae765f2a478eb58ab1ec25a67843","98dca3d669044181b9cd82ac0600f161","82c962fa4f754c4f8ba7f86424e31cdd","b1675717cf864bd597249a403255437c"]}},"execution_count":null,"outputs":[{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e9ae765f2a478eb58ab1ec25a67843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98dca3d669044181b9cd82ac0600f161"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/2 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82c962fa4f754c4f8ba7f86424e31cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/2 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1675717cf864bd597249a403255437c"}},"metadata":{}}]},{"cell_type":"code","source":["class ModelInference:\n","    def __init__(self):\n","        pass\n","\n","    def backwards_map_preds(self, sub_predictions, max_len, CFG):\n","        # nothing to map backwards if sequence is too short to be split in the first place\n","        if max_len != 1:\n","            for i in range(max_len):\n","                if i == 0:\n","                    # First sequence needs no SEP token (used to end a sequence)\n","                    sub_predictions = sub_predictions[:,:-1,:]\n","                elif i == max_len-1:\n","                    # End sequence needs to CLS token + Stride tokens\n","                    sub_predictions = sub_predictions[:,1+CFG.stride:,:] # CLS tokens + Stride tokens\n","                else:\n","                    # Middle sequence needs to CLS token + Stride tokens + SEP token\n","                    sub_predictions = sub_predictions[:,1+CFG.stride:-1,:]\n","\n","        return sub_predictions\n","\n","    def backwards_map_(self, row_attribute, max_len, CFG):\n","        # Same logics as for backwards_map_preds - except lists instead of 3darray\n","        if max_len != 1:\n","            for i in range(max_len):\n","                if i == 0:\n","                    row_attribute = row_attribute[:-1]\n","                elif i == max_len-1:\n","                    row_attribute = row_attribute[1+CFG.stride:]\n","                else:\n","                    row_attribute = row_attribute[1+CFG.stride:-1]\n","\n","        return row_attribute\n","\n","    def process_dataset(self, ds, tokenizer, CFG):\n","        # Initialize model and collator\n","        model = AutoModelForTokenClassification.from_pretrained(CFG.checkpoint)\n","        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n","\n","        # Define training arguments\n","        args = TrainingArguments(\n","            \".\",\n","            per_device_eval_batch_size=CFG.batch_size,\n","            report_to=\"none\",\n","        )\n","\n","        # Initialize Trainer\n","        trainer = Trainer(\n","            model=model,\n","            args=args,\n","            data_collator=collator,\n","            tokenizer=tokenizer,\n","        )\n","\n","        # Initialize empty lists and dictionary for storing predictions\n","        preds = []\n","        ds_dict = {\n","            \"document\":[],\n","            \"token_map\":[],\n","            \"offset_mapping\":[],\n","            \"tokens\":[]\n","        }\n","\n","        # Iterate over each row in the dataset\n","        for row in ds:\n","            # Initialize lists for storing predictions and offsets\n","            row_preds = []\n","            row_offset = []\n","\n","            # Iterate over tokens and their offset mappings in the row\n","            for i, y in enumerate(row[\"offset_mapping\"]):\n","                # Create a new dataset for each split of the document\n","                x = Dataset.from_dict({\n","                    \"token_type_ids\":[row[\"token_type_ids\"][i]],\n","                    \"input_ids\":[row[\"input_ids\"][i]],\n","                    \"attention_mask\":[row[\"attention_mask\"][i]],\n","                    \"offset_mapping\":[row[\"offset_mapping\"][i]]\n","                })\n","                # Predict for the split\n","                pred = trainer.predict(x).predictions\n","                # Remove stride and additional CLS & SEP tokens\n","                row_preds.append(self.backwards_map_preds(pred, len(row[\"offset_mapping\"]), CFG))\n","                row_offset += self.backwards_map_(y, len(row[\"offset_mapping\"]), CFG)\n","\n","            # Finalize row\n","            ds_dict[\"document\"].append(row[\"document\"])\n","            ds_dict[\"tokens\"].append(row[\"tokens\"])\n","            ds_dict[\"token_map\"].append(row[\"token_map\"])\n","            ds_dict[\"offset_mapping\"].append(row_offset)\n","\n","            # Finalize prediction collection by concatenating\n","            p_concat = np.concatenate(row_preds, axis = 1)\n","            preds.append(p_concat)\n","\n","        # Load model configuration\n","        config = json.load(open(Path(CFG.checkpoint) / \"config.json\"))\n","        id2label = config[\"id2label\"]\n","\n","        # Finalize predictions\n","        preds_final = []\n","        for predictions in preds:\n","            predictions_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n","            predictions = predictions.argmax(-1)\n","            predictions_without_O = predictions_softmax[:,:,:12].argmax(-1)\n","            O_predictions = predictions_softmax[:,:,12]\n","\n","            threshold = CFG.threshold\n","            preds_final.append(np.where(O_predictions < threshold, predictions_without_O , predictions))\n","\n","        # Create a new dataset from the dictionary\n","        ds = Dataset.from_dict(ds_dict)\n","\n","        # Initialize empty lists for storing pairs\n","        pairs = []\n","        document, token, label, token_str = [], [], [], []\n","\n","        # Iterate over predictions, token maps, offsets, tokens, and documents\n","        for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n","            # Iterate over each token prediction and its offset\n","            for token_pred, (start_idx, end_idx) in zip(p[0], offsets):\n","                label_pred = id2label[str(token_pred)]\n","\n","                if start_idx + end_idx == 0: continue\n","\n","                if token_map[start_idx] == -1:\n","                    start_idx += 1\n","\n","                # Ignore \"\\n\\n\"\n","                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n","                    start_idx += 1\n","\n","                if start_idx >= len(token_map): break\n","\n","                token_id = token_map[start_idx]\n","\n","                # Ignore \"O\" predictions and whitespace predictions\n","                if label_pred != \"O\" and token_id != -1:\n","                    pair=(doc, token_id)\n","\n","                    if pair not in pairs:\n","                        document.append(doc)\n","                        token.append(token_id)\n","                        label.append(label_pred)\n","                        token_str.append(tokens[token_id])\n","                        pairs.append(pair)\n","\n","        # Create DataFrame\n","        df = pd.DataFrame({\n","            \"document\": document,\n","            \"token\": token,\n","            \"label\": label,\n","            \"token_str\": token_str\n","        })\n","        df[\"row_id\"] = list(range(len(df)))\n","\n","        return df\n","\n","# Initialize ModelInference class\n","mi = ModelInference()"],"metadata":{"_kg_hide-input":true,"papermill":{"duration":0.044022,"end_time":"2024-03-24T08:41:34.682344","exception":false,"start_time":"2024-03-24T08:41:34.638322","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:58.748262Z","iopub.execute_input":"2024-05-10T02:38:58.748584Z","iopub.status.idle":"2024-05-10T02:38:58.778902Z","shell.execute_reply.started":"2024-05-10T02:38:58.748554Z","shell.execute_reply":"2024-05-10T02:38:58.777858Z"},"trusted":true,"id":"neZJ-32SsS8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process dataset and create DataFrame\n","df = mi.process_dataset(ds, tokenizer, CFG)"],"metadata":{"_kg_hide-output":true,"papermill":{"duration":6.906003,"end_time":"2024-03-24T08:41:41.60277","exception":false,"start_time":"2024-03-24T08:41:34.696767","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:38:58.780431Z","iopub.execute_input":"2024-05-10T02:38:58.780856Z","iopub.status.idle":"2024-05-10T02:39:06.273722Z","shell.execute_reply.started":"2024-05-10T02:38:58.780822Z","shell.execute_reply":"2024-05-10T02:39:06.272842Z"},"trusted":true,"id":"R5has8hcsS8P","outputId":"76211f3e-3eec-4f6c-f294-ea12438cea3e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":["# Display DataFrame\n","display(df)"],"metadata":{"papermill":{"duration":0.035826,"end_time":"2024-03-24T08:41:41.654895","exception":false,"start_time":"2024-03-24T08:41:41.619069","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T02:39:06.276354Z","iopub.execute_input":"2024-05-10T02:39:06.276659Z","iopub.status.idle":"2024-05-10T02:39:06.297488Z","shell.execute_reply.started":"2024-05-10T02:39:06.276634Z","shell.execute_reply":"2024-05-10T02:39:06.296464Z"},"trusted":true,"id":"SLUW3boosS8P","outputId":"524e305f-f13d-4bfa-dbd7-703286887916"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"    document  token           label  \\\n0          7      9  B-NAME_STUDENT   \n1          7     10  I-NAME_STUDENT   \n2          7    482  B-NAME_STUDENT   \n3          7    483  I-NAME_STUDENT   \n4          7    741  B-NAME_STUDENT   \n5          7    742  I-NAME_STUDENT   \n6         10      0  B-NAME_STUDENT   \n7         10      1  I-NAME_STUDENT   \n8         10    464  B-NAME_STUDENT   \n9         10    465  I-NAME_STUDENT   \n10        16      4  B-NAME_STUDENT   \n11        16      5  I-NAME_STUDENT   \n12        20      5  B-NAME_STUDENT   \n13        20      6  I-NAME_STUDENT   \n14        56     12  B-NAME_STUDENT   \n15        56     13  I-NAME_STUDENT   \n16        86      6  B-NAME_STUDENT   \n17        86      7  I-NAME_STUDENT   \n18        93      0  B-NAME_STUDENT   \n19        93      1  I-NAME_STUDENT   \n20       104      8  B-NAME_STUDENT   \n21       104      9  I-NAME_STUDENT   \n22       112      5  B-NAME_STUDENT   \n23       112      6  I-NAME_STUDENT   \n24       123     32  B-NAME_STUDENT   \n25       123     33  I-NAME_STUDENT   \n26       123   1500  B-NAME_STUDENT   \n27       123   1575  B-URL_PERSONAL   \n28       123   1581  B-NAME_STUDENT   \n\n                                     token_str  row_id  \n0                                     Nathalie       0  \n1                                        Sylla       1  \n2                                     Nathalie       2  \n3                                        Sylla       3  \n4                                     Nathalie       4  \n5                                        Sylla       5  \n6                                        Diego       6  \n7                                      Estrada       7  \n8                                        Diego       8  \n9                                      Estrada       9  \n10                                    Gilberto      10  \n11                                      Gamboa      11  \n12                                       Sindy      12  \n13                                      Samaca      13  \n14                                      Nadine      14  \n15                                        Born      15  \n16                                      Eladio      16  \n17                                       Amaya      17  \n18                                      Silvia      18  \n19                                  Villalobos      19  \n20                                       Sakir      20  \n21                                       Ahmad      21  \n22                                   Francisco      22  \n23                                    Ferreira      23  \n24                                     Stefano      24  \n25                                      Lovato      25  \n26                                  Kazantseva      26  \n27  https://cyberleninka.ru/article/n/14398333      27  \n28                                  Kazantseva      28  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Samaca</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nadine</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Born</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Stefano</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Lovato</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>123</td>\n      <td>1500</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Kazantseva</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>123</td>\n      <td>1575</td>\n      <td>B-URL_PERSONAL</td>\n      <td>https://cyberleninka.ru/article/n/14398333</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>123</td>\n      <td>1581</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Kazantseva</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["comp = pd.read_csv(\"/kaggle/input/output/output.csv\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-10T02:39:59.298816Z","iopub.execute_input":"2024-05-10T02:39:59.299540Z","iopub.status.idle":"2024-05-10T02:39:59.313038Z","shell.execute_reply.started":"2024-05-10T02:39:59.299506Z","shell.execute_reply":"2024-05-10T02:39:59.311946Z"},"trusted":true,"id":"WNpE7FvasS8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comp"],"metadata":{"execution":{"iopub.status.busy":"2024-05-10T02:40:03.816806Z","iopub.execute_input":"2024-05-10T02:40:03.817271Z","iopub.status.idle":"2024-05-10T02:40:03.832313Z","shell.execute_reply.started":"2024-05-10T02:40:03.817236Z","shell.execute_reply":"2024-05-10T02:40:03.831087Z"},"trusted":true,"id":"wTVECkVasS8Q","outputId":"14211987-6724-435f-a189-74831cc9473f"},"execution_count":null,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"    Token Position           label\n0                9  B-NAME_STUDENT\n1               10  I-NAME_STUDENT\n2              482  B-NAME_STUDENT\n3              483  I-NAME_STUDENT\n4              741  B-NAME_STUDENT\n5              742  I-NAME_STUDENT\n6                0  B-NAME_STUDENT\n7                1  I-NAME_STUDENT\n8              464  B-NAME_STUDENT\n9              465  I-NAME_STUDENT\n10               4  B-NAME_STUDENT\n11               5  I-NAME_STUDENT\n12               5  B-NAME_STUDENT\n13               6  I-NAME_STUDENT\n14              12  B-NAME_STUDENT\n15              13  I-NAME_STUDENT\n16               6  B-NAME_STUDENT\n17               7  I-NAME_STUDENT\n18               0  B-NAME_STUDENT\n19               1  I-NAME_STUDENT\n20               8  B-NAME_STUDENT\n21               9  I-NAME_STUDENT\n22               5  B-NAME_STUDENT\n23               6  I-NAME_STUDENT\n24              32  B-NAME_STUDENT\n25              33  I-NAME_STUDENT","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Token Position</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","\n","total_rows_df = len(df)\n","total_rows_comp = len(comp)\n","\n","# Initialize a counter for matching rows\n","matching_rows = 0\n","\n","# Iterate through rows in df\n","for index, row in df.iterrows():\n","    token_df = row['token']\n","    label_df = row['label']\n","\n","    # Iterate through rows in comp\n","    for index_comp, row_comp in comp.iterrows():\n","        token_comp = row_comp['Token Position']\n","        label_comp = row_comp['label']\n","\n","        # Compare 'Token' and 'Label' values\n","        if token_df == token_comp and label_df == label_comp:\n","            matching_rows += 1\n","            break  # Exit the inner loop if a match is found\n","\n","# Calculate accuracy\n","accuracy = (matching_rows / total_rows_df) * 100\n","\n","print(f\"Accuracy: {accuracy:.2f}%\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-10T02:43:26.457156Z","iopub.execute_input":"2024-05-10T02:43:26.458046Z","iopub.status.idle":"2024-05-10T02:43:26.499544Z","shell.execute_reply.started":"2024-05-10T02:43:26.458002Z","shell.execute_reply":"2024-05-10T02:43:26.498255Z"},"trusted":true,"id":"tgKRZ0RxsS8Q","outputId":"06ad6c44-f8ff-4da4-892f-9a8a03f67dc8"},"execution_count":null,"outputs":[{"name":"stdout","text":"Accuracy: 89.66%\n","output_type":"stream"}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assuming df and comp are your DataFrames\n","# Initialize variables for TP, FP, and FN\n","TP = 0\n","FP = 0\n","FN = 0\n","\n","# Iterate through rows in df\n","for index, row in df.iterrows():\n","    token_df = row['token']\n","    label_df = row['label']\n","\n","    # Initialize a flag for matching\n","    match_found = False\n","\n","    # Iterate through rows in comp\n","    for index_comp, row_comp in comp.iterrows():\n","        token_comp = row_comp['Token Position']\n","        label_comp = row_comp['label']\n","\n","        # Compare 'Token' and 'Label' values\n","        if token_df == token_comp and label_df == label_comp:\n","            match_found = True\n","            break  # Exit the inner loop if a match is found\n","\n","    # Update TP, FP, and FN counts\n","    if match_found:\n","        TP += 1\n","    else:\n","        FN += 1\n","\n","# Calculate FP as the difference between total rows in comp and TP\n","FP = total_rows_comp - TP\n","\n","# Calculate precision and recall\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-10T02:51:13.129340Z","iopub.execute_input":"2024-05-10T02:51:13.129748Z","iopub.status.idle":"2024-05-10T02:51:13.169364Z","shell.execute_reply.started":"2024-05-10T02:51:13.129721Z","shell.execute_reply":"2024-05-10T02:51:13.168388Z"},"trusted":true,"id":"M2g5-FDHsS8Q","outputId":"71185f37-fcb7-4399-89f2-5f7651ce4ceb"},"execution_count":null,"outputs":[{"name":"stdout","text":"Precision: 1.00\nRecall: 0.90\n","output_type":"stream"}]},{"cell_type":"code","source":["df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"],"metadata":{"papermill":{"duration":0.03195,"end_time":"2024-03-24T08:41:41.703211","exception":false,"start_time":"2024-03-24T08:41:41.671261","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T00:29:23.147845Z","iopub.status.idle":"2024-05-10T00:29:23.148188Z","shell.execute_reply.started":"2024-05-10T00:29:23.148024Z","shell.execute_reply":"2024-05-10T00:29:23.148038Z"},"trusted":true,"id":"fneUI7V0sS8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DltaKE9zsS8R"},"execution_count":null,"outputs":[]}]}